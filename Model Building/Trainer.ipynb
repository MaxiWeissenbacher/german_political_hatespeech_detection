{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f01e4f1",
   "metadata": {},
   "source": [
    "# Notebook of the Model Fine-Tuning with Huggingface Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca2afaf0-bac7-4531-ac91-e0a0141604af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install torchvision\n",
    "!pip install wandb\n",
    "!pip install ipynb\n",
    "!pip install s3fs\n",
    "!pip install nvidia-ml-py3\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bdb613ed-de25-4697-a053-42f56bbfc624",
   "metadata": {},
   "source": [
    "## Best Optuna Hyperparameters:\n",
    "    - BestRun(run_id='17', objective=0.8041917886879126, hyperparameters={'num_train_epochs': 3, 'learning_rate': 4.5132496745073475e-05, 'warmup_steps': 0.3, 'weight_decay': 0.022282597381446483, 'per_device_train_batch_size': 8})\n",
    "  - wie angegeben: 0.758063\n",
    "  - mit 7 Epochs anstatt 3: 0.784619\n",
    "  - mit 5 Epochs anstatt 3: 0.818966\n",
    "  - mit Batch 16 statt 8:  0.814296\n",
    "  - mit Batch 16 statt 8 und 6 Epochs: 0.770077\n",
    "  - mit Batch 16 statt 8 und 4 Epochs:0.818253\n",
    "  - mit Batch 16 statt 8 und 5 Epochs:0.814296\n",
    "        \n",
    "        \n",
    "\n",
    "## Best WANDB Hyperparameters:\n",
    "   - Batch Size: 4, Learning_Rate: 0.00002091, Epochs: 5, Warum_up: 0.3, Weight_Decay: 0.4\n",
    "   - wie angegeben: 0.789587\n",
    "   - mit Batch 8 statt 4: 0.801988\n",
    "   - mit Batch 16 statt 4: 0.799205\n",
    "   - mit Batch 8 statt 4 und 7 Epochs: 0.804964\n",
    "   - hier normale Werte verwenden und Argumentieren, dass die Parameter von Optuna besser geklappt haben\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f616ccee-46b1-49fc-a5b7-dd838ecea0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import EarlyStoppingCallback\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report \n",
    "from transformers import get_scheduler\n",
    "from transformers import AdamW\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import json\n",
    "import s3fs\n",
    "import os\n",
    "import torch\n",
    "from ipynb.fs.full.eval_metrics import *\n",
    "hello()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b7ec7b6-f2f8-4003-af60-cb74cbfda643",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "# Opening Credentials-JSON file\n",
    "with open('../credentials.json', 'r') as openfile:\n",
    " \n",
    "    # Reading from json file\n",
    "    json_object = json.load(openfile)\n",
    "    key = json_object[\"key\"]\n",
    "    secret = json_object[\"secret_key\"]\n",
    "    bucket_name = json_object[\"bucket_name\"]\n",
    "\n",
    "s3 = s3fs.S3FileSystem(anon=False,key=key,secret=secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d36b2b4-85e3-4d19-bc58-9bc1fb7527e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ansatz = \"ALLGERM_deepset_gbert_base\"\n",
    "model_name = \"deepset/gbert-base\"\n",
    "\n",
    "\n",
    "# Von BERT Models:\n",
    "learning_rate = 4.5132496745073475e-05\n",
    "epochs = 5\n",
    "weight_decay = 0.022282597381446483\n",
    "warmup = 0.3\n",
    "batch_size = 8\n",
    "\n",
    "train_test_number = [1,2,3,4,5]\n",
    "# Specify Approach with train_file_name and test_file_name\n",
    "#train_file_name = \"train.csv\"\n",
    "#test_file_name = \"test.csv\"\n",
    "#train_file_name = \"train_plus_neutral_germeval.csv\"\n",
    "#test_file_name = \"test_plus_neutral_germeval.csv\"\n",
    "#train_file_name = \"preprocessed_train_plus_neutral_germeval.csv\"\n",
    "#test_file_name = \"preprocessed_test_plus_neutral_germeval.csv\"\n",
    "train_file_name = \"train_plus_all_germeval.csv\"\n",
    "test_file_name = \"test_plus_all_germeval.csv\"\n",
    "\n",
    "# Listing all used models\n",
    "#Hate-speech-CNERG/dehatebert-mono-german                  ###DONE\n",
    "#shahrukhx01/gbert-hasoc-german-2019                       ###DONE\n",
    "#bert-base-multilingual-cased                              ###DONE\n",
    "#EIStakovskii/german_toxicity_classifier_plus_v2           ###DONE\n",
    "#\"bert-base-german-cased\"                                  ###DONE\n",
    "#\"dbmdz/bert-base-german-cased\"                            ###DONE --> Run 4 irgendwie voll schlecht\n",
    "#\"bert-base-german-dbmdz-uncased\"                          ###DONE\n",
    "#\"distilbert-base-german-cased\"                            ###DONE\n",
    "#\"german-nlp-group/electra-base-german-uncased\"            ###DONE\n",
    "#\"bert-base-multilingual-uncased\"                          ###DONE\n",
    "#\"xlm-roberta-base\"                                        ###DONE\n",
    "#T-Systems-onsite/cross-en-de-roberta-sentence-transformer ###DONE\n",
    "#cardiffnlp/twitter-xlm-roberta-base                       ###DONE\n",
    "# https://huggingface.co/cardiffnlp/twitter-roberta-base-offensive evtl für Translation Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13e11060-af41-41ab-960f-53ebcd255317",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold Number: 1\n",
      "                                                text  labels\n",
      "0  @Bundeskanzler @OlafScholz Deutschland muss ha...       0\n",
      "1  @ben_brechtken @OlafScholz Benny, du bist leid...       1\n",
      "10836\n",
      "2709\n",
      "Train Verteilung: 0    6901\n",
      "1    3935\n",
      "Name: labels, dtype: int64\n",
      "Test Verteilung: 0    1725\n",
      "1     984\n",
      "Name: labels, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/gbert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at deepset/gbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8668\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5420\n",
      "  Number of trainable parameters = 109929218\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvarrukta-mongo\u001b[0m (\u001b[33mmox\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/Modeling/wandb/run-20230307_085147-xm03e74h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mox/huggingface/runs/xm03e74h' target=\"_blank\">zany-terrain-83</a></strong> to <a href='https://wandb.ai/mox/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mox/huggingface' target=\"_blank\">https://wandb.ai/mox/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mox/huggingface/runs/xm03e74h' target=\"_blank\">https://wandb.ai/mox/huggingface/runs/xm03e74h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5420' max='5420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5420/5420 46:25, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.422600</td>\n",
       "      <td>0.412791</td>\n",
       "      <td>0.825646</td>\n",
       "      <td>0.822025</td>\n",
       "      <td>0.796120</td>\n",
       "      <td>0.805483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.310200</td>\n",
       "      <td>0.637444</td>\n",
       "      <td>0.821033</td>\n",
       "      <td>0.830909</td>\n",
       "      <td>0.779390</td>\n",
       "      <td>0.793791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.199600</td>\n",
       "      <td>0.841056</td>\n",
       "      <td>0.838100</td>\n",
       "      <td>0.830680</td>\n",
       "      <td>0.816438</td>\n",
       "      <td>0.822378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.071900</td>\n",
       "      <td>1.154563</td>\n",
       "      <td>0.831642</td>\n",
       "      <td>0.821452</td>\n",
       "      <td>0.812898</td>\n",
       "      <td>0.816699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>1.257715</td>\n",
       "      <td>0.826107</td>\n",
       "      <td>0.815866</td>\n",
       "      <td>0.805903</td>\n",
       "      <td>0.810235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2168\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to res/checkpoint-1084\n",
      "Configuration saved in res/checkpoint-1084/config.json\n",
      "Model weights saved in res/checkpoint-1084/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2168\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to res/checkpoint-2168\n",
      "Configuration saved in res/checkpoint-2168/config.json\n",
      "Model weights saved in res/checkpoint-2168/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2168\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to res/checkpoint-3252\n",
      "Configuration saved in res/checkpoint-3252/config.json\n",
      "Model weights saved in res/checkpoint-3252/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2168\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to res/checkpoint-4336\n",
      "Configuration saved in res/checkpoint-4336/config.json\n",
      "Model weights saved in res/checkpoint-4336/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2168\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to res/checkpoint-5420\n",
      "Configuration saved in res/checkpoint-5420/config.json\n",
      "Model weights saved in res/checkpoint-5420/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from res/checkpoint-3252 (score: 0.8223779987811024).\n",
      "Saving model checkpoint to ALLGERM_deepset_gbert_base_best_model\n",
      "Configuration saved in ALLGERM_deepset_gbert_base_best_model/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Best Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ALLGERM_deepset_gbert_base_best_model/pytorch_model.bin\n",
      "loading configuration file ALLGERM_deepset_gbert_base_best_model/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"ALLGERM_deepset_gbert_base_best_model\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "loading weights file ALLGERM_deepset_gbert_base_best_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ALLGERM_deepset_gbert_base_best_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2709\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              1\n",
      "0                              \n",
      "acc                    0.825028\n",
      "bal_acc                0.800400\n",
      "mcc                    0.615547\n",
      "f1_macro               0.806562\n",
      "f1_micro               0.825028\n",
      "f1_weighted            0.822910\n",
      "precision_macro        0.815327\n",
      "precision_micro        0.825028\n",
      "precision_weighted     0.823031\n",
      "recall_macro           0.800400\n",
      "recall_micro           0.825028\n",
      "recall_weighted        0.825028\n",
      "precision_class_0      0.843493\n",
      "precision_class_1      0.787162\n",
      "recall_class_0         0.890435\n",
      "recall_class_1         0.710366\n",
      "f1_score_class_0       0.866328\n",
      "f1_score_class_1       0.746795\n",
      "sample_class_0      1725.000000\n",
      "sample_class_1       984.000000\n",
      "Finished Fold Number: 1\n",
      "Fold Number: 2\n",
      "                                                text  labels\n",
      "0  Wenn es ums Geld aus Gas-, Öl- &amp; Kohlegesc...       1\n",
      "1  @JanSchipmann @emiliafester Sicher ?... ich em...       1\n",
      "10836\n",
      "2709\n",
      "Train Verteilung: 0    6901\n",
      "1    3935\n",
      "Name: labels, dtype: int64\n",
      "Test Verteilung: 0    1725\n",
      "1     984\n",
      "Name: labels, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/4a45e506eccc3405ed2e2a0502995d3f7e483509/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"deepset/gbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/4a45e506eccc3405ed2e2a0502995d3f7e483509/vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/4a45e506eccc3405ed2e2a0502995d3f7e483509/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/4a45e506eccc3405ed2e2a0502995d3f7e483509/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"deepset/gbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/4a45e506eccc3405ed2e2a0502995d3f7e483509/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"deepset/gbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/4a45e506eccc3405ed2e2a0502995d3f7e483509/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"deepset/gbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/4a45e506eccc3405ed2e2a0502995d3f7e483509/pytorch_model.bin\n",
      "Some weights of the model checkpoint at deepset/gbert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at deepset/gbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8668\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5420\n",
      "  Number of trainable parameters = 109929218\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5420' max='5420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5420/5420 48:06, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.426500</td>\n",
       "      <td>0.445629</td>\n",
       "      <td>0.799354</td>\n",
       "      <td>0.789504</td>\n",
       "      <td>0.763447</td>\n",
       "      <td>0.772652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.426600</td>\n",
       "      <td>0.662730</td>\n",
       "      <td>0.816882</td>\n",
       "      <td>0.800548</td>\n",
       "      <td>0.803998</td>\n",
       "      <td>0.802182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.145600</td>\n",
       "      <td>0.886688</td>\n",
       "      <td>0.813192</td>\n",
       "      <td>0.797628</td>\n",
       "      <td>0.794329</td>\n",
       "      <td>0.795903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.072400</td>\n",
       "      <td>1.214054</td>\n",
       "      <td>0.817804</td>\n",
       "      <td>0.810140</td>\n",
       "      <td>0.785193</td>\n",
       "      <td>0.794434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>1.352349</td>\n",
       "      <td>0.816882</td>\n",
       "      <td>0.805584</td>\n",
       "      <td>0.789284</td>\n",
       "      <td>0.795908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2168\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to res/checkpoint-1084\n",
      "Configuration saved in res/checkpoint-1084/config.json\n",
      "Model weights saved in res/checkpoint-1084/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2168\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to res/checkpoint-2168\n",
      "Configuration saved in res/checkpoint-2168/config.json\n",
      "Model weights saved in res/checkpoint-2168/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2168\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to res/checkpoint-3252\n",
      "Configuration saved in res/checkpoint-3252/config.json\n",
      "Model weights saved in res/checkpoint-3252/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2168\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to res/checkpoint-4336\n",
      "Configuration saved in res/checkpoint-4336/config.json\n",
      "Model weights saved in res/checkpoint-4336/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2168\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to res/checkpoint-5420\n",
      "Configuration saved in res/checkpoint-5420/config.json\n",
      "Model weights saved in res/checkpoint-5420/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from res/checkpoint-2168 (score: 0.8021817339367581).\n",
      "Saving model checkpoint to ALLGERM_deepset_gbert_base_best_model\n",
      "Configuration saved in ALLGERM_deepset_gbert_base_best_model/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Best Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ALLGERM_deepset_gbert_base_best_model/pytorch_model.bin\n",
      "loading configuration file ALLGERM_deepset_gbert_base_best_model/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"ALLGERM_deepset_gbert_base_best_model\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "loading weights file ALLGERM_deepset_gbert_base_best_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ALLGERM_deepset_gbert_base_best_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2709\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              1\n",
      "0                              \n",
      "acc                    0.825766\n",
      "bal_acc                0.807092\n",
      "mcc                    0.620221\n",
      "f1_macro               0.809883\n",
      "f1_micro               0.825766\n",
      "f1_weighted            0.824914\n",
      "precision_macro        0.813159\n",
      "precision_micro        0.825766\n",
      "precision_weighted     0.824482\n",
      "recall_macro           0.807092\n",
      "recall_micro           0.825766\n",
      "recall_weighted        0.825766\n",
      "precision_class_0      0.854556\n",
      "precision_class_1      0.771762\n",
      "recall_class_0         0.875362\n",
      "recall_class_1         0.738821\n",
      "f1_score_class_0       0.864834\n",
      "f1_score_class_1       0.754933\n",
      "sample_class_0      1725.000000\n",
      "sample_class_1       984.000000\n",
      "Finished Fold Number: 2\n",
      "Fold Number: 3\n",
      "                                                text  labels\n",
      "0  @Bundeskanzler @ABaerbock Was und wann schickt...       1\n",
      "1  @n_roettgen Verdammt. Jetzt wärt sich der Hund...       1\n",
      "10836\n",
      "2709\n",
      "Train Verteilung: 0    6901\n",
      "1    3935\n",
      "Name: labels, dtype: int64\n",
      "Test Verteilung: 0    1725\n",
      "1     984\n",
      "Name: labels, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/4a45e506eccc3405ed2e2a0502995d3f7e483509/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"deepset/gbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/4a45e506eccc3405ed2e2a0502995d3f7e483509/vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/4a45e506eccc3405ed2e2a0502995d3f7e483509/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/4a45e506eccc3405ed2e2a0502995d3f7e483509/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"deepset/gbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/4a45e506eccc3405ed2e2a0502995d3f7e483509/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"deepset/gbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/4a45e506eccc3405ed2e2a0502995d3f7e483509/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"deepset/gbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/4a45e506eccc3405ed2e2a0502995d3f7e483509/pytorch_model.bin\n",
      "Some weights of the model checkpoint at deepset/gbert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at deepset/gbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8668\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5420\n",
      "  Number of trainable parameters = 109929218\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5420' max='5420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5420/5420 47:43, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.508100</td>\n",
       "      <td>0.465564</td>\n",
       "      <td>0.799354</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.760900</td>\n",
       "      <td>0.771158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.471400</td>\n",
       "      <td>0.462851</td>\n",
       "      <td>0.816421</td>\n",
       "      <td>0.801945</td>\n",
       "      <td>0.795432</td>\n",
       "      <td>0.798411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.342000</td>\n",
       "      <td>0.626720</td>\n",
       "      <td>0.820572</td>\n",
       "      <td>0.805913</td>\n",
       "      <td>0.801782</td>\n",
       "      <td>0.803735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.165100</td>\n",
       "      <td>0.954705</td>\n",
       "      <td>0.817343</td>\n",
       "      <td>0.810159</td>\n",
       "      <td>0.783984</td>\n",
       "      <td>0.793550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.107000</td>\n",
       "      <td>0.985506</td>\n",
       "      <td>0.821033</td>\n",
       "      <td>0.813174</td>\n",
       "      <td>0.789691</td>\n",
       "      <td>0.798569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2168\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to res/checkpoint-1084\n",
      "Configuration saved in res/checkpoint-1084/config.json\n",
      "Model weights saved in res/checkpoint-1084/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2168\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to res/checkpoint-2168\n",
      "Configuration saved in res/checkpoint-2168/config.json\n",
      "Model weights saved in res/checkpoint-2168/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2168\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to res/checkpoint-3252\n",
      "Configuration saved in res/checkpoint-3252/config.json\n",
      "Model weights saved in res/checkpoint-3252/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2168\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to res/checkpoint-4336\n",
      "Configuration saved in res/checkpoint-4336/config.json\n",
      "Model weights saved in res/checkpoint-4336/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2168\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to res/checkpoint-5420\n",
      "Configuration saved in res/checkpoint-5420/config.json\n",
      "Model weights saved in res/checkpoint-5420/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from res/checkpoint-3252 (score: 0.8037346529519788).\n",
      "Saving model checkpoint to ALLGERM_deepset_gbert_base_best_model\n",
      "Configuration saved in ALLGERM_deepset_gbert_base_best_model/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Best Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ALLGERM_deepset_gbert_base_best_model/pytorch_model.bin\n",
      "loading configuration file ALLGERM_deepset_gbert_base_best_model/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"ALLGERM_deepset_gbert_base_best_model\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "loading weights file ALLGERM_deepset_gbert_base_best_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ALLGERM_deepset_gbert_base_best_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2709\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              1\n",
      "0                              \n",
      "acc                    0.821705\n",
      "bal_acc                0.804776\n",
      "mcc                    0.612733\n",
      "f1_macro               0.806299\n",
      "f1_micro               0.821705\n",
      "f1_weighted            0.821242\n",
      "precision_macro        0.807965\n",
      "precision_micro        0.821705\n",
      "precision_weighted     0.820902\n",
      "recall_macro           0.804776\n",
      "recall_micro           0.821705\n",
      "recall_weighted        0.821705\n",
      "precision_class_0      0.855263\n",
      "precision_class_1      0.760666\n",
      "recall_class_0         0.866667\n",
      "recall_class_1         0.742886\n",
      "f1_score_class_0       0.860927\n",
      "f1_score_class_1       0.751671\n",
      "sample_class_0      1725.000000\n",
      "sample_class_1       984.000000\n",
      "Finished Fold Number: 3\n",
      "Fold Number: 4\n",
      "                                                text  labels\n",
      "0  @spdde @larsklingbeil Ihr seid und bleibt Apol...       1\n",
      "1  @MutierMitMir @AfDVerdachtfall @SWagenknecht D...       1\n",
      "10836\n",
      "2709\n",
      "Train Verteilung: 0    6901\n",
      "1    3935\n",
      "Name: labels, dtype: int64\n",
      "Test Verteilung: 0    1725\n",
      "1     984\n",
      "Name: labels, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/4a45e506eccc3405ed2e2a0502995d3f7e483509/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"deepset/gbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/4a45e506eccc3405ed2e2a0502995d3f7e483509/vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/4a45e506eccc3405ed2e2a0502995d3f7e483509/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/4a45e506eccc3405ed2e2a0502995d3f7e483509/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"deepset/gbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/4a45e506eccc3405ed2e2a0502995d3f7e483509/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"deepset/gbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/4a45e506eccc3405ed2e2a0502995d3f7e483509/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"deepset/gbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/4a45e506eccc3405ed2e2a0502995d3f7e483509/pytorch_model.bin\n",
      "Some weights of the model checkpoint at deepset/gbert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at deepset/gbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8668\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5420\n",
      "  Number of trainable parameters = 109929218\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5420' max='5420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5420/5420 47:38, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.506100</td>\n",
       "      <td>0.481713</td>\n",
       "      <td>0.787362</td>\n",
       "      <td>0.782915</td>\n",
       "      <td>0.756187</td>\n",
       "      <td>0.764537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.461800</td>\n",
       "      <td>0.458002</td>\n",
       "      <td>0.803044</td>\n",
       "      <td>0.791959</td>\n",
       "      <td>0.787148</td>\n",
       "      <td>0.789338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.330600</td>\n",
       "      <td>0.633618</td>\n",
       "      <td>0.807657</td>\n",
       "      <td>0.795882</td>\n",
       "      <td>0.805190</td>\n",
       "      <td>0.799355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.210900</td>\n",
       "      <td>0.733906</td>\n",
       "      <td>0.804889</td>\n",
       "      <td>0.793087</td>\n",
       "      <td>0.792393</td>\n",
       "      <td>0.792735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.125700</td>\n",
       "      <td>0.860601</td>\n",
       "      <td>0.802583</td>\n",
       "      <td>0.793252</td>\n",
       "      <td>0.782316</td>\n",
       "      <td>0.786793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2168\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to res/checkpoint-1084\n",
      "Configuration saved in res/checkpoint-1084/config.json\n",
      "Model weights saved in res/checkpoint-1084/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2168\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to res/checkpoint-2168\n",
      "Configuration saved in res/checkpoint-2168/config.json\n",
      "Model weights saved in res/checkpoint-2168/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2168\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to res/checkpoint-3252\n",
      "Configuration saved in res/checkpoint-3252/config.json\n",
      "Model weights saved in res/checkpoint-3252/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2168\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to res/checkpoint-4336\n",
      "Configuration saved in res/checkpoint-4336/config.json\n",
      "Model weights saved in res/checkpoint-4336/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2168\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to res/checkpoint-5420\n",
      "Configuration saved in res/checkpoint-5420/config.json\n",
      "Model weights saved in res/checkpoint-5420/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from res/checkpoint-3252 (score: 0.7993547346131371).\n",
      "Saving model checkpoint to ALLGERM_deepset_gbert_base_best_model\n",
      "Configuration saved in ALLGERM_deepset_gbert_base_best_model/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Best Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ALLGERM_deepset_gbert_base_best_model/pytorch_model.bin\n",
      "loading configuration file ALLGERM_deepset_gbert_base_best_model/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"ALLGERM_deepset_gbert_base_best_model\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "loading weights file ALLGERM_deepset_gbert_base_best_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ALLGERM_deepset_gbert_base_best_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2709\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              1\n",
      "0                              \n",
      "acc                    0.792543\n",
      "bal_acc                0.784934\n",
      "mcc                    0.560823\n",
      "f1_macro               0.779579\n",
      "f1_micro               0.792543\n",
      "f1_weighted            0.794201\n",
      "precision_macro        0.775961\n",
      "precision_micro        0.792543\n",
      "precision_weighted     0.797405\n",
      "recall_macro           0.784934\n",
      "recall_micro           0.792543\n",
      "recall_weighted        0.792543\n",
      "precision_class_0      0.854357\n",
      "precision_class_1      0.697566\n",
      "recall_class_0         0.812754\n",
      "recall_class_1         0.757114\n",
      "f1_score_class_0       0.833036\n",
      "f1_score_class_1       0.726121\n",
      "sample_class_0      1725.000000\n",
      "sample_class_1       984.000000\n",
      "Finished Fold Number: 4\n",
      "Fold Number: 5\n",
      "                                                text  labels\n",
      "0  @SWagenknecht Die Deutschen werden bis heute f...       0\n",
      "1  @MarcoBuschmann @DasRechtsportal Mann!! Bushma...       1\n",
      "10836\n",
      "2709\n",
      "Train Verteilung: 0    6900\n",
      "1    3936\n",
      "Name: labels, dtype: int64\n",
      "Test Verteilung: 0    1726\n",
      "1     983\n",
      "Name: labels, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/4a45e506eccc3405ed2e2a0502995d3f7e483509/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"deepset/gbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/4a45e506eccc3405ed2e2a0502995d3f7e483509/vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/4a45e506eccc3405ed2e2a0502995d3f7e483509/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/4a45e506eccc3405ed2e2a0502995d3f7e483509/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"deepset/gbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/4a45e506eccc3405ed2e2a0502995d3f7e483509/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"deepset/gbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/4a45e506eccc3405ed2e2a0502995d3f7e483509/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"deepset/gbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/jovyan/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/4a45e506eccc3405ed2e2a0502995d3f7e483509/pytorch_model.bin\n",
      "Some weights of the model checkpoint at deepset/gbert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at deepset/gbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8668\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5420\n",
      "  Number of trainable parameters = 109929218\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5420' max='5420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5420/5420 45:48, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.450700</td>\n",
       "      <td>0.531395</td>\n",
       "      <td>0.787823</td>\n",
       "      <td>0.778358</td>\n",
       "      <td>0.801114</td>\n",
       "      <td>0.780576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.350900</td>\n",
       "      <td>0.678741</td>\n",
       "      <td>0.813653</td>\n",
       "      <td>0.798982</td>\n",
       "      <td>0.818066</td>\n",
       "      <td>0.804381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.177100</td>\n",
       "      <td>0.860155</td>\n",
       "      <td>0.830720</td>\n",
       "      <td>0.814877</td>\n",
       "      <td>0.820184</td>\n",
       "      <td>0.817328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.072300</td>\n",
       "      <td>0.968628</td>\n",
       "      <td>0.829336</td>\n",
       "      <td>0.814818</td>\n",
       "      <td>0.812238</td>\n",
       "      <td>0.813486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>1.227687</td>\n",
       "      <td>0.827952</td>\n",
       "      <td>0.815133</td>\n",
       "      <td>0.806009</td>\n",
       "      <td>0.810084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2168\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to res/checkpoint-1084\n",
      "Configuration saved in res/checkpoint-1084/config.json\n",
      "Model weights saved in res/checkpoint-1084/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2168\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to res/checkpoint-2168\n",
      "Configuration saved in res/checkpoint-2168/config.json\n",
      "Model weights saved in res/checkpoint-2168/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2168\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to res/checkpoint-3252\n",
      "Configuration saved in res/checkpoint-3252/config.json\n",
      "Model weights saved in res/checkpoint-3252/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2168\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to res/checkpoint-4336\n",
      "Configuration saved in res/checkpoint-4336/config.json\n",
      "Model weights saved in res/checkpoint-4336/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2168\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to res/checkpoint-5420\n",
      "Configuration saved in res/checkpoint-5420/config.json\n",
      "Model weights saved in res/checkpoint-5420/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from res/checkpoint-3252 (score: 0.8173280482499188).\n",
      "Saving model checkpoint to ALLGERM_deepset_gbert_base_best_model\n",
      "Configuration saved in ALLGERM_deepset_gbert_base_best_model/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Best Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ALLGERM_deepset_gbert_base_best_model/pytorch_model.bin\n",
      "loading configuration file ALLGERM_deepset_gbert_base_best_model/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"ALLGERM_deepset_gbert_base_best_model\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "loading weights file ALLGERM_deepset_gbert_base_best_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ALLGERM_deepset_gbert_base_best_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2709\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              1\n",
      "0                              \n",
      "acc                    0.819860\n",
      "bal_acc                0.806520\n",
      "mcc                    0.611462\n",
      "f1_macro               0.805713\n",
      "f1_micro               0.819860\n",
      "f1_weighted            0.820092\n",
      "precision_macro        0.804944\n",
      "precision_micro        0.819860\n",
      "precision_weighted     0.820358\n",
      "recall_macro           0.806520\n",
      "recall_micro           0.819860\n",
      "recall_weighted        0.819860\n",
      "precision_class_0      0.861144\n",
      "precision_class_1      0.748744\n",
      "recall_class_0         0.855156\n",
      "recall_class_1         0.757884\n",
      "f1_score_class_0       0.858140\n",
      "f1_score_class_1       0.753286\n",
      "sample_class_0      1726.000000\n",
      "sample_class_1       983.000000\n",
      "Finished Fold Number: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# do the 5-Fold Cross Validation (Datasets are splitted already and stored in a S3 Bucket)\n",
    "for train_index in train_test_number:\n",
    "    print(f\"Fold Number: {train_index}\")\n",
    "    # Read data\n",
    "    with s3.open(f\"{bucket_name}/KFOLD{train_index}/{train_file_name}\",'r') as file:\n",
    "        data = pd.read_csv(file)\n",
    "    with s3.open(f\"{bucket_name}/KFOLD{train_index}/{test_file_name}\",'r') as file:\n",
    "        test_data = pd.read_csv(file)\n",
    "    data = data[[\"Text\", \"majority_vote\"]]\n",
    "    test_data = test_data[[\"Text\", \"majority_vote\"]]\n",
    "    data.rename(columns={'Text': 'text', 'majority_vote': 'labels'}, inplace=True)\n",
    "    test_data.rename(columns={'Text': 'text', 'majority_vote': 'labels'}, inplace=True)\n",
    "    print(test_data.head(2))\n",
    "    print(len(data))\n",
    "    print(len(test_data))\n",
    "    print(f\"Train Verteilung: {data.labels.value_counts()}\")\n",
    "    print(f\"Test Verteilung: {test_data.labels.value_counts()}\")\n",
    "\n",
    "    # Define pretrained tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "    # ----- 1. Preprocess data -----#\n",
    "    # Preprocess data\n",
    "    X = list(data[\"text\"])\n",
    "    y = list(data[\"labels\"])\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=11)\n",
    "    X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n",
    "    X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    # Create torch dataset\n",
    "    class Dataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, encodings, labels=None):\n",
    "            self.encodings = encodings\n",
    "            self.labels = labels\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "            if self.labels:\n",
    "                item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "            return item\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "    train_dataset = Dataset(X_train_tokenized, y_train)\n",
    "    val_dataset = Dataset(X_val_tokenized, y_val)\n",
    "    \n",
    "    # ----- 2. Fine-tune pretrained model -----#\n",
    "    \n",
    "    # Define Trainer parameters\n",
    "    def compute_metrics(p):\n",
    "        pred, labels = p\n",
    "        pred = np.argmax(pred, axis=1)\n",
    "\n",
    "        accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "        recall = recall_score(y_true=labels, y_pred=pred,average=\"macro\")\n",
    "        precision = precision_score(y_true=labels, y_pred=pred,average=\"macro\")\n",
    "        f1 = f1_score(y_true=labels, y_pred=pred,average=\"macro\")\n",
    "\n",
    "        return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        report_to=\"none\", # disable wandb\n",
    "        output_dir=f\"res\",\n",
    "        overwrite_output_dir=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        eval_steps=50,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=50,\n",
    "        save_strategy=\"epoch\",\n",
    "        #save_steps=200,\n",
    "        learning_rate= learning_rate,\n",
    "        warmup_steps =  warmup,\n",
    "        weight_decay = weight_decay,\n",
    "        per_device_train_batch_size= batch_size,\n",
    "        per_device_eval_batch_size= batch_size,\n",
    "        num_train_epochs= epochs,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n",
    "    )\n",
    "\n",
    "    # Train pre-trained model\n",
    "    trainer.train()\n",
    "    \n",
    "    print(\"Saving Best Model\")\n",
    "    trainer.save_model(f\"{ansatz}_best_model\")\n",
    "\n",
    "    # ----- 3. Predict -----#\n",
    "    # Load test data\n",
    "    X_test = list(test_data[\"text\"])\n",
    "    X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    # Create torch dataset\n",
    "    test_dataset = Dataset(X_test_tokenized)\n",
    "\n",
    "    # Load trained model\n",
    "    #best_ckpt_path = trainer.state.best_model_checkpoint\n",
    "    #model_path = \"gbert_res/checkpoint-200\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(f\"{ansatz}_best_model\", num_labels=2)\n",
    "\n",
    "    # Define test trainer\n",
    "    test_trainer = Trainer(model)\n",
    "    #test_trainer = trainer\n",
    "\n",
    "    # Make prediction\n",
    "    raw_pred, _, _ = test_trainer.predict(test_dataset)\n",
    "\n",
    "    # Preprocess raw predictions\n",
    "    y_pred = np.argmax(raw_pred, axis=1)\n",
    "\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    test_list = test_data[\"labels\"].tolist()\n",
    "    test_list = np.array(test_list)\n",
    "\n",
    "    # Compute Evaluation Metrics\n",
    "    f = f1_multiclass(test_list, y_pred)\n",
    "    p = p_multiclass(test_list, y_pred)\n",
    "    r = recall_multiclass(test_list, y_pred)\n",
    "    a = accuracy_score(test_list, y_pred)\n",
    "    ba = balanced_accuracy_score(test_list, y_pred)\n",
    "    prs = precision_recall_fscore_support(test_list, y_pred)\n",
    "    m = matthews_corrcoef(test_list,y_pred)\n",
    "\n",
    "    results = {}\n",
    "    results[\"acc\"] = a\n",
    "    results[\"f1\"] = f\n",
    "    results[\"precision\"] = p\n",
    "    results[\"recall\"] = r\n",
    "    results[\"bal_acc\"] = ba\n",
    "    results[\"prfs\"] = prs\n",
    "    results[\"mcc\"] = m\n",
    "\n",
    "    prepare_results(results)\n",
    "    result_df = dict_to_df(results)\n",
    "    result_df.to_csv(f\"{ansatz}{train_index}.csv\")\n",
    "    print(result_df)\n",
    "    class_rep = classification_report(test_list, y_pred,target_names=[\"HOF\", \"NOT\"])\n",
    "    cm = confusion_matrix(test_list, y_pred,)\n",
    "    ax = plt.subplot()\n",
    "    cm_plot = sns.heatmap(cm, annot=True, fmt='g', ax=ax,cmap='Greens');\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title(F'Confusion Matrix of Run {train_index}') \n",
    "    ax.xaxis.set_ticklabels(['NOT', 'HOF'])\n",
    "    ax.yaxis.set_ticklabels(['NOT', 'HOF']);\n",
    "    cm_plot.figure.savefig(f\"{ansatz}_best_model/confusion_matrix_{train_index}.png\")\n",
    "    print(f\"Finished Fold Number: {train_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d09c752e-1f0b-4e05-a1b5-ce4ce196968c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metrics</th>\n",
       "      <th>Overall Results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acc</td>\n",
       "      <td>0.816980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bal_acc</td>\n",
       "      <td>0.800744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mcc</td>\n",
       "      <td>0.604157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f1_macro</td>\n",
       "      <td>0.801607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f1_micro</td>\n",
       "      <td>0.816980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>f1_weighted</td>\n",
       "      <td>0.816672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>precision_macro</td>\n",
       "      <td>0.803471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>precision_micro</td>\n",
       "      <td>0.816980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>precision_weighted</td>\n",
       "      <td>0.817236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>recall_macro</td>\n",
       "      <td>0.800744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>recall_micro</td>\n",
       "      <td>0.816980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>recall_weighted</td>\n",
       "      <td>0.816980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>precision_class_0</td>\n",
       "      <td>0.853762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>precision_class_1</td>\n",
       "      <td>0.753180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>recall_class_0</td>\n",
       "      <td>0.860075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>recall_class_1</td>\n",
       "      <td>0.741414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>f1_score_class_0</td>\n",
       "      <td>0.856653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>f1_score_class_1</td>\n",
       "      <td>0.746561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sample_class_0</td>\n",
       "      <td>1725.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sample_class_1</td>\n",
       "      <td>983.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Metrics  Overall Results\n",
       "0                  acc         0.816980\n",
       "1              bal_acc         0.800744\n",
       "2                  mcc         0.604157\n",
       "3             f1_macro         0.801607\n",
       "4             f1_micro         0.816980\n",
       "5          f1_weighted         0.816672\n",
       "6      precision_macro         0.803471\n",
       "7      precision_micro         0.816980\n",
       "8   precision_weighted         0.817236\n",
       "9         recall_macro         0.800744\n",
       "10        recall_micro         0.816980\n",
       "11     recall_weighted         0.816980\n",
       "12   precision_class_0         0.853762\n",
       "13   precision_class_1         0.753180\n",
       "14      recall_class_0         0.860075\n",
       "15      recall_class_1         0.741414\n",
       "16    f1_score_class_0         0.856653\n",
       "17    f1_score_class_1         0.746561\n",
       "18      sample_class_0      1725.200000\n",
       "19      sample_class_1       983.800000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread SystemMonitor:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.9/threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 118, in _start\n",
      "    asset.start()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/wandb/sdk/internal/system/assets/cpu.py\", line 166, in start\n",
      "    self.metrics_monitor.start()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/wandb/sdk/internal/system/assets/interfaces.py\", line 168, in start\n",
      "    logger.info(f\"Started {self._process.name}\")\n",
      "AttributeError: 'NoneType' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "# Compute Average Scores out of the results of all Folds\n",
    "res = kfold_evaluation(ansatz)\n",
    "res.to_csv(f\"KFOLD_PREPROCESS_Model_Results/{ansatz}.csv\")\n",
    "res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f129d001-d818-4cf3-87fa-01abf25667e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Speicherplatz verwalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7ac9562-08e2-47ef-9144-167c218798a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory: 11811160064\n",
      "Free memory: 11719409664\n",
      "Used memory: 91750400\n"
     ]
    }
   ],
   "source": [
    "import nvidia_smi\n",
    "\n",
    "nvidia_smi.nvmlInit()\n",
    "\n",
    "handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "# card id 0 hardcoded here, there is also a call to get all available card ids, so we could iterate\n",
    "\n",
    "info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "\n",
    "print(\"Total memory:\", info.total)\n",
    "print(\"Free memory:\", info.free)\n",
    "print(\"Used memory:\", info.used)\n",
    "\n",
    "nvidia_smi.nvmlShutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75b6d6c4-489f-4596-b358-23b286a19155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear Cuda Cache, if it is out of memory\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
