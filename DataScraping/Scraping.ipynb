{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3026db8a-5f4e-47dd-9461-c837827f437b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweepy\n",
      "  Using cached tweepy-4.12.1-py3-none-any.whl (101 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in /opt/conda/lib/python3.9/site-packages (from tweepy) (2.28.1)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /opt/conda/lib/python3.9/site-packages (from tweepy) (1.3.1)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in /opt/conda/lib/python3.9/site-packages (from tweepy) (3.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy) (1.26.9)\n",
      "Installing collected packages: tweepy\n",
      "Successfully installed tweepy-4.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1a5ea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "#create a twitter_authentication file on your own and store your bearer token in it\n",
    "from twitter_authentication import bearer_token\n",
    "import time\n",
    "import pandas as pd\n",
    "import time\n",
    "# Reference: https://github.com/jdfoote/Intro-to-Programming-and-Data-Science/blob/fall2021/extra_topics/twitter_v2_example.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff4946fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tweepy.Client(bearer_token, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9420e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "740"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politicians = pd.read_csv(\"twitter_usernames_extracted.csv\", sep=\";\")\n",
    "politicians = politicians.drop([\"Unnamed: 0\", \"Unnamed: 0.1\"], axis=1)\n",
    "len(politicians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "817ef044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "523\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_name</th>\n",
       "      <th>party</th>\n",
       "      <th>twitter</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sanae Abdi</td>\n",
       "      <td>SPD</td>\n",
       "      <td>https://twitter.com/abdisanae']</td>\n",
       "      <td>abdisanae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Valentin Abel</td>\n",
       "      <td>FDP</td>\n",
       "      <td>['https://twitter.com/Valentin_C_Abel']</td>\n",
       "      <td>Valentin_C_Abel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Katja Adler</td>\n",
       "      <td>FDP</td>\n",
       "      <td>['https://twitter.com/katjadler']</td>\n",
       "      <td>katjadler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stephanie Aeffner</td>\n",
       "      <td>B√ºndnis 90/Die Gr√ºnen</td>\n",
       "      <td>['https://twitter.com/s_aeffner']</td>\n",
       "      <td>s_aeffner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>G√∂kay Akbulut</td>\n",
       "      <td>Die Linke</td>\n",
       "      <td>['https://twitter.com/akbulutgokay?lang=de']</td>\n",
       "      <td>akbulutgokay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>Kay-Uwe Ziegler</td>\n",
       "      <td>AfD</td>\n",
       "      <td>['https://twitter.com/KayUweZiegler71']</td>\n",
       "      <td>KayUweZiegler71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>Paul Ziemiak</td>\n",
       "      <td>CDU/CSU</td>\n",
       "      <td>['https://twitter.com/PaulZiemiak']</td>\n",
       "      <td>PaulZiemiak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>Stefan Zierke</td>\n",
       "      <td>SPD</td>\n",
       "      <td>['http://twitter.com/zierke']</td>\n",
       "      <td>zierke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>Dr. Jens Zimmermann</td>\n",
       "      <td>SPD</td>\n",
       "      <td>['https://twitter.com/jenszspd']</td>\n",
       "      <td>jenszspd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>Nicolas Zippelius</td>\n",
       "      <td>CDU/CSU</td>\n",
       "      <td>['https://twitter.com/nzippelius?lang=de']</td>\n",
       "      <td>nzippelius</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>523 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               full_name                   party  \\\n",
       "0             Sanae Abdi                     SPD   \n",
       "1          Valentin Abel                     FDP   \n",
       "3            Katja Adler                     FDP   \n",
       "4      Stephanie Aeffner   B√ºndnis 90/Die Gr√ºnen   \n",
       "6          G√∂kay Akbulut               Die Linke   \n",
       "..                   ...                     ...   \n",
       "735      Kay-Uwe Ziegler                     AfD   \n",
       "736         Paul Ziemiak                 CDU/CSU   \n",
       "737        Stefan Zierke                     SPD   \n",
       "738  Dr. Jens Zimmermann                     SPD   \n",
       "739    Nicolas Zippelius                 CDU/CSU   \n",
       "\n",
       "                                          twitter         username  \n",
       "0                 https://twitter.com/abdisanae']        abdisanae  \n",
       "1         ['https://twitter.com/Valentin_C_Abel']  Valentin_C_Abel  \n",
       "3               ['https://twitter.com/katjadler']        katjadler  \n",
       "4               ['https://twitter.com/s_aeffner']        s_aeffner  \n",
       "6    ['https://twitter.com/akbulutgokay?lang=de']     akbulutgokay  \n",
       "..                                            ...              ...  \n",
       "735       ['https://twitter.com/KayUweZiegler71']  KayUweZiegler71  \n",
       "736           ['https://twitter.com/PaulZiemiak']      PaulZiemiak  \n",
       "737                 ['http://twitter.com/zierke']           zierke  \n",
       "738              ['https://twitter.com/jenszspd']         jenszspd  \n",
       "739    ['https://twitter.com/nzippelius?lang=de']       nzippelius  \n",
       "\n",
       "[523 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter only for politcians with an active Twitter Account\n",
    "politicians = politicians.loc[politicians.twitter != \"[]\"]\n",
    "print(len(politicians))\n",
    "politicians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45763770-da1a-4750-9dda-1b6d62723f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " SPD                      144\n",
       " B√ºndnis 90/Die Gr√ºnen    106\n",
       " CDU/CSU                  102\n",
       " FDP                       81\n",
       " AfD                       53\n",
       " Die Linke                 32\n",
       " fraktionslos               5\n",
       "Name: party, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politicians.party.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "845bf6c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gruene = politicians.loc[politicians.party == \" B√ºndnis 90/Die Gr√ºnen\"]\n",
    "len(gruene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "677e3c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Man muss wegen Twitter richtlinien in 40er Schritten gehen\n",
    "#SPD_1 = SPD[0:40]\n",
    "#SPD_2 = SPD[40:80]\n",
    "#SPD_3 = SPD[80:120]\n",
    "#SPD_4 = SPD[120:144]\n",
    "#afd_1 = afd[0:25]\n",
    "#afd_2 = afd[25:54]\n",
    "#fdp_1 = fdp[0:41]\n",
    "#fdp_2 = fdp[41:82]\n",
    "gruene_1 = gruene[0:40]\n",
    "gruene_2 = gruene[40:80]\n",
    "gruene_3 = gruene[80:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31db00b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SPD_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m spd_polis_1 \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m user \u001b[38;5;129;01min\u001b[39;00m \u001b[43mSPD_1\u001b[49m\u001b[38;5;241m.\u001b[39musername:\n\u001b[1;32m      4\u001b[0m     user \u001b[38;5;241m=\u001b[39m user\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     spd_polis_1\u001b[38;5;241m.\u001b[39mappend(user)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SPD_1' is not defined"
     ]
    }
   ],
   "source": [
    "spd_polis_1 = []\n",
    "\n",
    "for user in SPD_1.username:\n",
    "    user = user.replace(\"@\",\"\")\n",
    "    spd_polis_1.append(user)\n",
    "\n",
    "spd_polis_2 = []\n",
    "\n",
    "for user in SPD_2.username:\n",
    "    user = user.replace(\"@\",\"\")\n",
    "    spd_polis_2.append(user)\n",
    "    \n",
    "spd_polis_3 = []\n",
    "\n",
    "for user in SPD_3.username:\n",
    "    user = user.replace(\"@\",\"\")\n",
    "    spd_polis_3.append(user)\n",
    "    \n",
    "spd_polis_4 = []\n",
    "\n",
    "for user in SPD_4.username:\n",
    "    user = user.replace(\"@\",\"\")\n",
    "    spd_polis_4.append(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ab85f6b6-f40b-4f5b-8d53-f92e267aac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "afd_polis = []\n",
    "\n",
    "for user in afd_1.username:\n",
    "    user = user.replace(\"@\",\"\")\n",
    "    afd_polis.append(user)\n",
    "    \n",
    "afd_polis_1 = []\n",
    "\n",
    "for user in afd_2.username:\n",
    "    user = user.replace(\"@\",\"\")\n",
    "    afd_polis_1.append(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31719aff-1517-4afd-8d7b-e18b9db288f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdp_polis = []\n",
    "\n",
    "for user in fdp_1.username:\n",
    "    user = user.replace(\"@\",\"\")\n",
    "    fdp_polis.append(user)\n",
    "    \n",
    "fdp_polis_1 = []\n",
    "\n",
    "for user in fdp_2.username:\n",
    "    user = user.replace(\"@\",\"\")\n",
    "    fdp_polis_1.append(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84acd782-cdd5-4d01-98dc-ca29212d6979",
   "metadata": {},
   "outputs": [],
   "source": [
    "gruene_polis_1 = []\n",
    "\n",
    "for user in gruene_1.username:\n",
    "    user = user.replace(\"@\",\"\")\n",
    "    gruene_polis_1.append(user)\n",
    "    \n",
    "gruene_polis_2 = []\n",
    "\n",
    "for user in gruene_2.username:\n",
    "    user = user.replace(\"@\",\"\")\n",
    "    gruene_polis_2.append(user)\n",
    "\n",
    "gruene_polis_3 = []\n",
    "\n",
    "for user in gruene_3.username:\n",
    "    user = user.replace(\"@\",\"\")\n",
    "    gruene_polis_3.append(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2b768ce-a1a9-40aa-a6f1-fec4b887e430",
   "metadata": {},
   "outputs": [],
   "source": [
    "linke_polis = []\n",
    "\n",
    "for user in linke.username:\n",
    "    user = user.replace(\"@\",\"\")\n",
    "    linke_polis.append(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bae6c6d",
   "metadata": {},
   "source": [
    "# Liste von Usern scrapen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "84f94849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Macht es Sinn den Wahlbeobachter Account zu Scrapen? Da m√ºssten alle Tweets drinnen sein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81ce9c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from:Schwarz_MdB OR from:rischwasu OR from:lina_seitzl OR from:stadler_svenja OR from:stamm_fibich OR from:ralf_stegner OR from:stonie_kiel OR from:nadjasthamer OR from:stuewer OR from:michaelthews OR from:Toens_NRW04 OR from:CarstenTraeger OR from:anja_troff OR from:derya_tn OR from:marjavoellers OR from:krawallstein OR from:HannesWalterSPD OR from:CarmenWegge OR from:drjoeweingarten OR from:thereallenzii OR from:BerndWestphal4 OR from:dirkwiesespd OR from:zierke OR from:jenszspd'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spd_polis_1 = \" OR \".join([f\"from:{username}\" for username in spd_polis_1])\n",
    "spd_polis_2 = \" OR \".join([f\"from:{username}\" for username in spd_polis_2])\n",
    "spd_polis_3 = \" OR \".join([f\"from:{username}\" for username in spd_polis_3])\n",
    "spd_polis_4 = \" OR \".join([f\"from:{username}\" for username in spd_polis_4])\n",
    "spd_polis_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5b7151ec-4ffc-4962-bee3-5187ba48d283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from:DrChristinaBaum OR from:BerndBaumannAfD OR from:RogerBeckamp OR from:MarcBernhardAfD OR from:AndreasBleckMdB OR from:PeterBoehringer OR from:stbrandner OR from:JuergenBraunAfD OR from:Marcus_Buehl OR from:PetrBystronAfD OR from:Tino_Chrupalla OR from:GottfriedCurio OR from:EspendillerM OR from:gtzfrmming OR from:Hannes_Gnauck OR from:M_HarderKuehnel OR from:Jochen_Haug OR from:martin_hess_afd OR from:Karsten_Hilse OR from:Nicole_Hoechst OR from:Leif_Erik_Holm OR from:GerritHuy OR from:marc_jongen OR from:MalteKaufmann OR from:KaufmannAfD'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "afd_polis = \" OR \".join([f\"from:{username}\" for username in afd_polis])\n",
    "afd_polis_1 = \" OR \".join([f\"from:{username}\" for username in afd_polis_1])\n",
    "afd_polis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "107ec39a-b72b-48ab-801d-232bae68949b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from:Valentin_C_Abel OR from:katjadler OR from:halakmuhanad OR from:RenataAlt_MdB OR from:cad59 OR from:nicole_ae_bauer OR from:jensbeeck OR from:IngoBodtke OR from:f_boginski OR from:JBrandenburgFDP OR from:brafdp OR from:MarcoBuschmann OR from:cjcronenberg OR from:DjirSarai OR from:christianduerr OR from:marcusfaber OR from:dfoest OR from:Otto_Fricke OR from:max_fksr OR from:MartinGassner OR from:AnikoMerten OR from:nilsgruender OR from:hacker_fdp OR from:philipphartewig OR from:HarzerUlrike OR from:PeterHeidtFDP OR from:KatrinHelling OR from:HerbrandMarkus OR from:torstenherbst OR from:fdp_hessel OR from:GeroHocker OR from:ManuelHoeferlin OR from:HoffmannForest OR from:HoubenReinhard OR from:gydej OR from:MdBKlein OR from:DanielaKluckert OR from:pascalkober OR from:koehler_fdp OR from:carina_konrad OR from:krusehamburg'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdp_polis = \" OR \".join([f\"from:{username}\" for username in fdp_polis])\n",
    "fdp_polis_1 = \" OR \".join([f\"from:{username}\" for username in fdp_polis_1])\n",
    "fdp_polis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6feb57f3-c22a-414a-8bb4-8fe1f23f0b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "linke_polis = \" OR \".join([f\"from:{username}\" for username in linke_polis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5a52e66-f0f4-4e96-aa37-ba05ace0fa15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from:akbulutgokay OR from:DietmarBartsch OR from:MWBirkwald OR from:C_AB_ OR from:SevimDagdelen OR from:anked OR from:ernst_klaus OR from:SusanneFerschl OR from:goerke_c OR from:nicolegohlke OR from:AtesGuerpinar OR from:GregorGysi OR from:SusanneHennig OR from:andrejhunko OR from:katjakipping OR from:jankortemdb OR from:christian_leye OR from:LoetzschMdB OR from:pascalmeiser OR from:Conni_Moehring OR from:Amira_M_Ali OR from:zaklinnastic OR from:PetraPauMaHe OR from:linkepelli OR from:victorperli OR from:HeidiReichinnek OR from:martinarenner OR from:b_riexinger OR from:petra_sitte_mdb OR from:jessica_tatti OR from:voglerk OR from:SWagenknecht'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linke_polis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d3a9085-3065-4a19-ac30-a8b259497788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from:sebastian_es_ OR from:ulle_schauws OR from:Schmidt_MdB OR from:lime_green_leni OR from:cj_schroeder OR from:k_sa OR from:Melis_S OR from:nyke_slawik OR from:AnneSpallek OR from:spellerberg_m OR from:ninastahr OR from:till_steffen OR from:HanSteinmueller OR from:w_sk OR from:kassem_ts OR from:AwetTesfaiesus OR from:JTrittin OR from:katrinuhligbn OR from:julia_verlinden OR from:wagenerniklas OR from:robinwagener OR from:yooHannes OR from:BeateWaRo OR from:SaskiaLeaRaquel OR from:stefwenzel OR from:TinaWinklmann'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gruene_polis_1 = \" OR \".join([f\"from:{username}\" for username in gruene_polis_1])\n",
    "gruene_polis_2 = \" OR \".join([f\"from:{username}\" for username in gruene_polis_2])\n",
    "gruene_polis_3 = \" OR \".join([f\"from:{username}\" for username in gruene_polis_3])\n",
    "gruene_polis_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "369e64b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 456 seconds.\n",
      "Rate limit exceeded. Sleeping for 508 seconds.\n",
      "Rate limit exceeded. Sleeping for 505 seconds.\n",
      "Rate limit exceeded. Sleeping for 507 seconds.\n",
      "Rate limit exceeded. Sleeping for 507 seconds.\n",
      "Rate limit exceeded. Sleeping for 508 seconds.\n",
      "Rate limit exceeded. Sleeping for 508 seconds.\n",
      "Rate limit exceeded. Sleeping for 508 seconds.\n",
      "Rate limit exceeded. Sleeping for 506 seconds.\n",
      "Rate limit exceeded. Sleeping for 505 seconds.\n",
      "Rate limit exceeded. Sleeping for 501 seconds.\n",
      "Rate limit exceeded. Sleeping for 508 seconds.\n",
      "Rate limit exceeded. Sleeping for 504 seconds.\n",
      "Rate limit exceeded. Sleeping for 503 seconds.\n",
      "Rate limit exceeded. Sleeping for 505 seconds.\n",
      "Rate limit exceeded. Sleeping for 505 seconds.\n",
      "Rate limit exceeded. Sleeping for 505 seconds.\n",
      "Rate limit exceeded. Sleeping for 500 seconds.\n",
      "Rate limit exceeded. Sleeping for 510 seconds.\n",
      "Rate limit exceeded. Sleeping for 507 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Scrapen von hat 294.7245960156123 Minuten gebraucht.\n",
      "5780\n"
     ]
    }
   ],
   "source": [
    "# time counter\n",
    "start = time.time()\n",
    "\n",
    "spd_tweets = []\n",
    "\n",
    "# exclude retweets, just retrieve German Tweets\n",
    "# Zeit ist YYMMDD\n",
    "# Soll ich Zeitraum seit Corona nehmen? also ab 01.01.2020? \n",
    "for response in tweepy.Paginator(client.search_all_tweets, \n",
    "                                query = f'({gruene_polis_2}) -is:retweet lang:de',\n",
    "                                user_fields = ['username', 'public_metrics', 'description', 'location'],\n",
    "                                tweet_fields = ['created_at', 'geo', 'public_metrics', 'text'],\n",
    "                                expansions = 'author_id',\n",
    "                                start_time = '2020-01-01T00:00:00Z',\n",
    "                                end_time = '2022-12-05T00:00:00Z'):\n",
    "    time.sleep(1)\n",
    "    spd_tweets.append(response)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Das Scrapen von hat {(end - start)/60} Minuten gebraucht.\")\n",
    "print(len(spd_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4005116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5780"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spd_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ce4f4f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "user_dict = {}\n",
    "# Loop through each response object\n",
    "for response in spd_tweets:\n",
    "    # Take all of the users, and put them into a dictionary of dictionaries with the info we want to keep\n",
    "    for user in response.includes['users']:\n",
    "        user_dict[user.id] = {'username': user.username, \n",
    "                              'followers': user.public_metrics['followers_count'],\n",
    "                              'tweets': user.public_metrics['tweet_count'],\n",
    "                              'description': user.description,\n",
    "                              'location': user.location\n",
    "                             }\n",
    "    for tweet in response.data:\n",
    "        # For each tweet, find the author's information\n",
    "        author_info = user_dict[tweet.author_id]\n",
    "        # Put all of the information we want to keep in a single dictionary for each tweet\n",
    "        result.append({'author_id': tweet.author_id, \n",
    "                       'username': author_info['username'],\n",
    "                       'author_followers': author_info['followers'],\n",
    "                       'author_tweets': author_info['tweets'],\n",
    "                       'author_description': author_info['description'],\n",
    "                       'author_location': author_info['location'],\n",
    "                       'text': tweet.text,\n",
    "                       'created_at': tweet.created_at,\n",
    "                       'quote_count': tweet.public_metrics['quote_count'],\n",
    "                       'retweets': tweet.public_metrics['retweet_count'],\n",
    "                       'replies': tweet.public_metrics['reply_count'],\n",
    "                       'likes': tweet.public_metrics['like_count'],\n",
    "                      })\n",
    "\n",
    "# Change this list of dictionaries into a dataframe\n",
    "df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8567a0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Partei\"] = \"B√ºndnis 90/Die Gr√ºnen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "366463c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>username</th>\n",
       "      <th>author_followers</th>\n",
       "      <th>author_tweets</th>\n",
       "      <th>author_description</th>\n",
       "      <th>author_location</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>retweets</th>\n",
       "      <th>replies</th>\n",
       "      <th>likes</th>\n",
       "      <th>Partei</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52318801</td>\n",
       "      <td>PaulaPiechotta</td>\n",
       "      <td>8998</td>\n",
       "      <td>16792</td>\n",
       "      <td>Evidence based politics. | √Ñrztin MD MSc | F√ºr...</td>\n",
       "      <td>LEIPZIG</td>\n",
       "      <td>@gruenerjoseph @ABaerbock Weil es Teil des Wah...</td>\n",
       "      <td>2022-12-04 20:53:40+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>B√ºndnis 90/Die Gr√ºnen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81596596</td>\n",
       "      <td>SaraNanni</td>\n",
       "      <td>9834</td>\n",
       "      <td>19613</td>\n",
       "      <td>üåª MdB, Sicherheitspolitische Sprecherin B90/Gr...</td>\n",
       "      <td>D√ºsseldorf und Berlin</td>\n",
       "      <td>@politik_felix Da es hier Ulm gesellschaftspol...</td>\n",
       "      <td>2022-12-04 20:49:53+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>B√ºndnis 90/Die Gr√ºnen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>81596596</td>\n",
       "      <td>SaraNanni</td>\n",
       "      <td>9834</td>\n",
       "      <td>19613</td>\n",
       "      <td>üåª MdB, Sicherheitspolitische Sprecherin B90/Gr...</td>\n",
       "      <td>D√ºsseldorf und Berlin</td>\n",
       "      <td>Vor Jahren machte die Gr√ºne Jugend Werbung f√ºr...</td>\n",
       "      <td>2022-12-04 20:39:27+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>82</td>\n",
       "      <td>B√ºndnis 90/Die Gr√ºnen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>81596596</td>\n",
       "      <td>SaraNanni</td>\n",
       "      <td>9834</td>\n",
       "      <td>19613</td>\n",
       "      <td>üåª MdB, Sicherheitspolitische Sprecherin B90/Gr...</td>\n",
       "      <td>D√ºsseldorf und Berlin</td>\n",
       "      <td>@BendlerBlogger Hab es ganz anders verstanden‚Ä¶...</td>\n",
       "      <td>2022-12-04 18:32:31+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>B√ºndnis 90/Die Gr√ºnen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>81596596</td>\n",
       "      <td>SaraNanni</td>\n",
       "      <td>9834</td>\n",
       "      <td>19613</td>\n",
       "      <td>üåª MdB, Sicherheitspolitische Sprecherin B90/Gr...</td>\n",
       "      <td>D√ºsseldorf und Berlin</td>\n",
       "      <td>@Team_Luftwaffe @ThielsChristian Dann br√§uchte...</td>\n",
       "      <td>2022-12-04 18:29:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>29</td>\n",
       "      <td>B√ºndnis 90/Die Gr√ºnen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57793</th>\n",
       "      <td>1405886484</td>\n",
       "      <td>Ricarda_Lang</td>\n",
       "      <td>128024</td>\n",
       "      <td>15268</td>\n",
       "      <td>Bundesvorsitzende von @die_Gruenen. Abgeordnet...</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Bei Klimaschutz geht es nicht einfach nur um √ñ...</td>\n",
       "      <td>2020-01-01 11:38:51+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>173</td>\n",
       "      <td>B√ºndnis 90/Die Gr√ºnen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57794</th>\n",
       "      <td>16706236</td>\n",
       "      <td>svenlehmann</td>\n",
       "      <td>35415</td>\n",
       "      <td>23707</td>\n",
       "      <td>üèõ Mitglied des Bundestages üó≥ K√∂ln-S√ºdwest üåª Pa...</td>\n",
       "      <td>K√∂ln/Berlin</td>\n",
       "      <td>@cameracaf1 @WDR Deswegen: https://t.co/bNLabv...</td>\n",
       "      <td>2020-01-01 11:32:13+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>B√ºndnis 90/Die Gr√ºnen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57795</th>\n",
       "      <td>1405886484</td>\n",
       "      <td>Ricarda_Lang</td>\n",
       "      <td>128024</td>\n",
       "      <td>15268</td>\n",
       "      <td>Bundesvorsitzende von @die_Gruenen. Abgeordnet...</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Wenn wir einfach so weiter machen wie bisher, ...</td>\n",
       "      <td>2020-01-01 11:12:49+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>6</td>\n",
       "      <td>182</td>\n",
       "      <td>B√ºndnis 90/Die Gr√ºnen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57796</th>\n",
       "      <td>1425265488</td>\n",
       "      <td>RenateKuenast</td>\n",
       "      <td>84153</td>\n",
       "      <td>42297</td>\n",
       "      <td>Feministin, Foodie, G√§rtnerin. Anw√§ltin, Sozia...</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>#B√∂llern! Gute Nacht. https://t.co/np6sz572T2</td>\n",
       "      <td>2020-01-01 02:33:16+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>B√ºndnis 90/Die Gr√ºnen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57797</th>\n",
       "      <td>1425265488</td>\n",
       "      <td>RenateKuenast</td>\n",
       "      <td>84153</td>\n",
       "      <td>42297</td>\n",
       "      <td>Feministin, Foodie, G√§rtnerin. Anw√§ltin, Sozia...</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>@HartmutZiebs üòé Wir finden Ihre Arbeit trotzde...</td>\n",
       "      <td>2020-01-01 01:02:20+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>B√ºndnis 90/Die Gr√ºnen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57798 rows √ó 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        author_id        username  author_followers  author_tweets  \\\n",
       "0        52318801  PaulaPiechotta              8998          16792   \n",
       "1        81596596       SaraNanni              9834          19613   \n",
       "2        81596596       SaraNanni              9834          19613   \n",
       "3        81596596       SaraNanni              9834          19613   \n",
       "4        81596596       SaraNanni              9834          19613   \n",
       "...           ...             ...               ...            ...   \n",
       "57793  1405886484    Ricarda_Lang            128024          15268   \n",
       "57794    16706236     svenlehmann             35415          23707   \n",
       "57795  1405886484    Ricarda_Lang            128024          15268   \n",
       "57796  1425265488   RenateKuenast             84153          42297   \n",
       "57797  1425265488   RenateKuenast             84153          42297   \n",
       "\n",
       "                                      author_description  \\\n",
       "0      Evidence based politics. | √Ñrztin MD MSc | F√ºr...   \n",
       "1      üåª MdB, Sicherheitspolitische Sprecherin B90/Gr...   \n",
       "2      üåª MdB, Sicherheitspolitische Sprecherin B90/Gr...   \n",
       "3      üåª MdB, Sicherheitspolitische Sprecherin B90/Gr...   \n",
       "4      üåª MdB, Sicherheitspolitische Sprecherin B90/Gr...   \n",
       "...                                                  ...   \n",
       "57793  Bundesvorsitzende von @die_Gruenen. Abgeordnet...   \n",
       "57794  üèõ Mitglied des Bundestages üó≥ K√∂ln-S√ºdwest üåª Pa...   \n",
       "57795  Bundesvorsitzende von @die_Gruenen. Abgeordnet...   \n",
       "57796  Feministin, Foodie, G√§rtnerin. Anw√§ltin, Sozia...   \n",
       "57797  Feministin, Foodie, G√§rtnerin. Anw√§ltin, Sozia...   \n",
       "\n",
       "             author_location  \\\n",
       "0                    LEIPZIG   \n",
       "1      D√ºsseldorf und Berlin   \n",
       "2      D√ºsseldorf und Berlin   \n",
       "3      D√ºsseldorf und Berlin   \n",
       "4      D√ºsseldorf und Berlin   \n",
       "...                      ...   \n",
       "57793                 Berlin   \n",
       "57794            K√∂ln/Berlin   \n",
       "57795                 Berlin   \n",
       "57796                 Berlin   \n",
       "57797                 Berlin   \n",
       "\n",
       "                                                    text  \\\n",
       "0      @gruenerjoseph @ABaerbock Weil es Teil des Wah...   \n",
       "1      @politik_felix Da es hier Ulm gesellschaftspol...   \n",
       "2      Vor Jahren machte die Gr√ºne Jugend Werbung f√ºr...   \n",
       "3      @BendlerBlogger Hab es ganz anders verstanden‚Ä¶...   \n",
       "4      @Team_Luftwaffe @ThielsChristian Dann br√§uchte...   \n",
       "...                                                  ...   \n",
       "57793  Bei Klimaschutz geht es nicht einfach nur um √ñ...   \n",
       "57794  @cameracaf1 @WDR Deswegen: https://t.co/bNLabv...   \n",
       "57795  Wenn wir einfach so weiter machen wie bisher, ...   \n",
       "57796      #B√∂llern! Gute Nacht. https://t.co/np6sz572T2   \n",
       "57797  @HartmutZiebs üòé Wir finden Ihre Arbeit trotzde...   \n",
       "\n",
       "                     created_at  quote_count  retweets  replies  likes  \\\n",
       "0     2022-12-04 20:53:40+00:00            0         2        1      5   \n",
       "1     2022-12-04 20:49:53+00:00            0         0        0      9   \n",
       "2     2022-12-04 20:39:27+00:00            0        10        4     82   \n",
       "3     2022-12-04 18:32:31+00:00            0         1        2     13   \n",
       "4     2022-12-04 18:29:00+00:00            0         4       18     29   \n",
       "...                         ...          ...       ...      ...    ...   \n",
       "57793 2020-01-01 11:38:51+00:00            0        31        3    173   \n",
       "57794 2020-01-01 11:32:13+00:00            0         0        0      2   \n",
       "57795 2020-01-01 11:12:49+00:00            1        33        6    182   \n",
       "57796 2020-01-01 02:33:16+00:00            0         2        2     27   \n",
       "57797 2020-01-01 01:02:20+00:00            0         0        0     27   \n",
       "\n",
       "                      Partei  \n",
       "0      B√ºndnis 90/Die Gr√ºnen  \n",
       "1      B√ºndnis 90/Die Gr√ºnen  \n",
       "2      B√ºndnis 90/Die Gr√ºnen  \n",
       "3      B√ºndnis 90/Die Gr√ºnen  \n",
       "4      B√ºndnis 90/Die Gr√ºnen  \n",
       "...                      ...  \n",
       "57793  B√ºndnis 90/Die Gr√ºnen  \n",
       "57794  B√ºndnis 90/Die Gr√ºnen  \n",
       "57795  B√ºndnis 90/Die Gr√ºnen  \n",
       "57796  B√ºndnis 90/Die Gr√ºnen  \n",
       "57797  B√ºndnis 90/Die Gr√ºnen  \n",
       "\n",
       "[57798 rows x 13 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=['created_at'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e0165d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57798"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)\n",
    "#sind das zu weninge? allein bei cdu top 10 warens 19k\n",
    "# dann 18k bei SPD_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "42ebf11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"gruene_zweite_40.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b8df3e-18d1-48d7-83ed-36199f4da587",
   "metadata": {},
   "source": [
    "# Gr√ºne Dritte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b92e4c42-a761-4dfb-9ebd-5b6795cae947",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 501 seconds.\n",
      "Rate limit exceeded. Sleeping for 502 seconds.\n",
      "Rate limit exceeded. Sleeping for 504 seconds.\n",
      "Rate limit exceeded. Sleeping for 499 seconds.\n",
      "Rate limit exceeded. Sleeping for 501 seconds.\n",
      "Rate limit exceeded. Sleeping for 500 seconds.\n",
      "Rate limit exceeded. Sleeping for 503 seconds.\n",
      "Rate limit exceeded. Sleeping for 508 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Scrapen von hat 121.39635774294536 Minuten gebraucht.\n",
      "2458\n",
      "24580\n"
     ]
    }
   ],
   "source": [
    "# time counter\n",
    "start = time.time()\n",
    "\n",
    "spd_tweets = []\n",
    "\n",
    "# exclude retweets, just retrieve German Tweets\n",
    "# Zeit ist YYMMDD\n",
    "# Soll ich Zeitraum seit Corona nehmen? also ab 01.01.2020? \n",
    "for response in tweepy.Paginator(client.search_all_tweets, \n",
    "                                query = f'({gruene_polis_3}) -is:retweet lang:de',\n",
    "                                user_fields = ['username', 'public_metrics', 'description', 'location'],\n",
    "                                tweet_fields = ['created_at', 'geo', 'public_metrics', 'text'],\n",
    "                                expansions = 'author_id',\n",
    "                                start_time = '2020-01-01T00:00:00Z',\n",
    "                                end_time = '2022-12-05T00:00:00Z'):\n",
    "    time.sleep(1)\n",
    "    spd_tweets.append(response)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Das Scrapen von hat {(end - start)/60} Minuten gebraucht.\")\n",
    "print(len(spd_tweets))\n",
    "\n",
    "result = []\n",
    "user_dict = {}\n",
    "# Loop through each response object\n",
    "for response in spd_tweets:\n",
    "    # Take all of the users, and put them into a dictionary of dictionaries with the info we want to keep\n",
    "    for user in response.includes['users']:\n",
    "        user_dict[user.id] = {'username': user.username, \n",
    "                              'followers': user.public_metrics['followers_count'],\n",
    "                              'tweets': user.public_metrics['tweet_count'],\n",
    "                              'description': user.description,\n",
    "                              'location': user.location\n",
    "                             }\n",
    "    for tweet in response.data:\n",
    "        # For each tweet, find the author's information\n",
    "        author_info = user_dict[tweet.author_id]\n",
    "        # Put all of the information we want to keep in a single dictionary for each tweet\n",
    "        result.append({'author_id': tweet.author_id, \n",
    "                       'username': author_info['username'],\n",
    "                       'author_followers': author_info['followers'],\n",
    "                       'author_tweets': author_info['tweets'],\n",
    "                       'author_description': author_info['description'],\n",
    "                       'author_location': author_info['location'],\n",
    "                       'text': tweet.text,\n",
    "                       'created_at': tweet.created_at,\n",
    "                       'quote_count': tweet.public_metrics['quote_count'],\n",
    "                       'retweets': tweet.public_metrics['retweet_count'],\n",
    "                       'replies': tweet.public_metrics['reply_count'],\n",
    "                       'likes': tweet.public_metrics['like_count'],\n",
    "                      })\n",
    "\n",
    "# Change this list of dictionaries into a dataframe\n",
    "df = pd.DataFrame(result)\n",
    "\n",
    "df[\"Partei\"] = \"B√ºndnis 90/Die Gr√ºnen\"\n",
    "\n",
    "df.to_csv(\"gruene_dritte_40.csv\")\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe7b342-0bbe-4f14-b6f9-4ec525100030",
   "metadata": {},
   "source": [
    "# CSU/CDU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cf05673-3ec2-40c6-a583-c51333eabb09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(csu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af552e48-1c08-4e65-bfee-824118ff470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "csu = politicians.loc[politicians.party == \" CDU/CSU\"]\n",
    "\n",
    "csu_1 = csu[0:40]\n",
    "csu_2 = csu[40:80]\n",
    "csu_3 = csu[80:]\n",
    "\n",
    "csu_polis_1 = []\n",
    "\n",
    "for user in csu_1.username:\n",
    "    user = user.replace(\"@\",\"\")\n",
    "    csu_polis_1.append(user)\n",
    "    \n",
    "csu_polis_2 = []\n",
    "\n",
    "for user in csu_2.username:\n",
    "    user = user.replace(\"@\",\"\")\n",
    "    csu_polis_2.append(user)\n",
    "\n",
    "csu_polis_3 = []\n",
    "\n",
    "for user in csu_3.username:\n",
    "    user = user.replace(\"@\",\"\")\n",
    "    csu_polis_3.append(user)\n",
    "    \n",
    "csu_polis_1 = \" OR \".join([f\"from:{username}\" for username in csu_polis_1])\n",
    "csu_polis_2 = \" OR \".join([f\"from:{username}\" for username in csu_polis_2])\n",
    "csu_polis_3 = \" OR \".join([f\"from:{username}\" for username in csu_polis_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a1247e9-392d-4d99-8e3c-036fd237b487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 509 seconds.\n",
      "Rate limit exceeded. Sleeping for 512 seconds.\n",
      "Rate limit exceeded. Sleeping for 510 seconds.\n",
      "Rate limit exceeded. Sleeping for 512 seconds.\n",
      "Rate limit exceeded. Sleeping for 513 seconds.\n",
      "Rate limit exceeded. Sleeping for 514 seconds.\n",
      "Rate limit exceeded. Sleeping for 515 seconds.\n",
      "Rate limit exceeded. Sleeping for 514 seconds.\n",
      "Rate limit exceeded. Sleeping for 514 seconds.\n",
      "Rate limit exceeded. Sleeping for 517 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Scrapen von hat 156.54759421348572 Minuten gebraucht.\n",
      "3299\n",
      "32981\n"
     ]
    }
   ],
   "source": [
    "# time counter\n",
    "start = time.time()\n",
    "\n",
    "spd_tweets = []\n",
    "\n",
    "# exclude retweets, just retrieve German Tweets\n",
    "# Zeit ist YYMMDD\n",
    "# Soll ich Zeitraum seit Corona nehmen? also ab 01.01.2020? \n",
    "for response in tweepy.Paginator(client.search_all_tweets, \n",
    "                                query = f'({csu_polis_1}) -is:retweet lang:de',\n",
    "                                user_fields = ['username', 'public_metrics', 'description', 'location'],\n",
    "                                tweet_fields = ['created_at', 'geo', 'public_metrics', 'text'],\n",
    "                                expansions = 'author_id',\n",
    "                                start_time = '2020-01-01T00:00:00Z',\n",
    "                                end_time = '2022-12-05T00:00:00Z'):\n",
    "    time.sleep(1)\n",
    "    spd_tweets.append(response)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Das Scrapen von hat {(end - start)/60} Minuten gebraucht.\")\n",
    "print(len(spd_tweets))\n",
    "\n",
    "result = []\n",
    "user_dict = {}\n",
    "# Loop through each response object\n",
    "for response in spd_tweets:\n",
    "    # Take all of the users, and put them into a dictionary of dictionaries with the info we want to keep\n",
    "    for user in response.includes['users']:\n",
    "        user_dict[user.id] = {'username': user.username, \n",
    "                              'followers': user.public_metrics['followers_count'],\n",
    "                              'tweets': user.public_metrics['tweet_count'],\n",
    "                              'description': user.description,\n",
    "                              'location': user.location\n",
    "                             }\n",
    "    for tweet in response.data:\n",
    "        # For each tweet, find the author's information\n",
    "        author_info = user_dict[tweet.author_id]\n",
    "        # Put all of the information we want to keep in a single dictionary for each tweet\n",
    "        result.append({'author_id': tweet.author_id, \n",
    "                       'username': author_info['username'],\n",
    "                       'author_followers': author_info['followers'],\n",
    "                       'author_tweets': author_info['tweets'],\n",
    "                       'author_description': author_info['description'],\n",
    "                       'author_location': author_info['location'],\n",
    "                       'text': tweet.text,\n",
    "                       'created_at': tweet.created_at,\n",
    "                       'quote_count': tweet.public_metrics['quote_count'],\n",
    "                       'retweets': tweet.public_metrics['retweet_count'],\n",
    "                       'replies': tweet.public_metrics['reply_count'],\n",
    "                       'likes': tweet.public_metrics['like_count'],\n",
    "                      })\n",
    "\n",
    "# Change this list of dictionaries into a dataframe\n",
    "df = pd.DataFrame(result)\n",
    "\n",
    "df[\"Partei\"] = \"CSU/CDU\"\n",
    "\n",
    "df.to_csv(\"csucdu_erste_40.csv\")\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c69454-fd8f-40fb-825e-0b6522820b1d",
   "metadata": {},
   "source": [
    "# CSU Zweite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab58006c-a866-4d21-8d15-13f9bf07fe10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 517 seconds.\n",
      "Rate limit exceeded. Sleeping for 503 seconds.\n",
      "Rate limit exceeded. Sleeping for 504 seconds.\n",
      "Rate limit exceeded. Sleeping for 507 seconds.\n",
      "Rate limit exceeded. Sleeping for 510 seconds.\n",
      "Rate limit exceeded. Sleeping for 509 seconds.\n",
      "Rate limit exceeded. Sleeping for 512 seconds.\n",
      "Rate limit exceeded. Sleeping for 509 seconds.\n",
      "Rate limit exceeded. Sleeping for 510 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Scrapen von hat 129.9506334622701 Minuten gebraucht.\n",
      "2455\n",
      "24544\n"
     ]
    }
   ],
   "source": [
    "# time counter\n",
    "start = time.time()\n",
    "\n",
    "spd_tweets = []\n",
    "\n",
    "# exclude retweets, just retrieve German Tweets\n",
    "# Zeit ist YYMMDD\n",
    "# Soll ich Zeitraum seit Corona nehmen? also ab 01.01.2020? \n",
    "for response in tweepy.Paginator(client.search_all_tweets, \n",
    "                                query = f'({csu_polis_2}) -is:retweet lang:de',\n",
    "                                user_fields = ['username', 'public_metrics', 'description', 'location'],\n",
    "                                tweet_fields = ['created_at', 'geo', 'public_metrics', 'text'],\n",
    "                                expansions = 'author_id',\n",
    "                                start_time = '2020-01-01T00:00:00Z',\n",
    "                                end_time = '2022-12-05T00:00:00Z'):\n",
    "    time.sleep(1)\n",
    "    spd_tweets.append(response)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Das Scrapen von hat {(end - start)/60} Minuten gebraucht.\")\n",
    "print(len(spd_tweets))\n",
    "\n",
    "result = []\n",
    "user_dict = {}\n",
    "# Loop through each response object\n",
    "for response in spd_tweets:\n",
    "    # Take all of the users, and put them into a dictionary of dictionaries with the info we want to keep\n",
    "    for user in response.includes['users']:\n",
    "        user_dict[user.id] = {'username': user.username, \n",
    "                              'followers': user.public_metrics['followers_count'],\n",
    "                              'tweets': user.public_metrics['tweet_count'],\n",
    "                              'description': user.description,\n",
    "                              'location': user.location\n",
    "                             }\n",
    "    for tweet in response.data:\n",
    "        # For each tweet, find the author's information\n",
    "        author_info = user_dict[tweet.author_id]\n",
    "        # Put all of the information we want to keep in a single dictionary for each tweet\n",
    "        result.append({'author_id': tweet.author_id, \n",
    "                       'username': author_info['username'],\n",
    "                       'author_followers': author_info['followers'],\n",
    "                       'author_tweets': author_info['tweets'],\n",
    "                       'author_description': author_info['description'],\n",
    "                       'author_location': author_info['location'],\n",
    "                       'text': tweet.text,\n",
    "                       'created_at': tweet.created_at,\n",
    "                       'quote_count': tweet.public_metrics['quote_count'],\n",
    "                       'retweets': tweet.public_metrics['retweet_count'],\n",
    "                       'replies': tweet.public_metrics['reply_count'],\n",
    "                       'likes': tweet.public_metrics['like_count'],\n",
    "                      })\n",
    "\n",
    "# Change this list of dictionaries into a dataframe\n",
    "df = pd.DataFrame(result)\n",
    "\n",
    "df[\"Partei\"] = \"CSU/CDU\"\n",
    "\n",
    "df.to_csv(\"csucdu_zweite_40.csv\")\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ca8865-f679-4845-b251-ebd112429f28",
   "metadata": {},
   "source": [
    "# CSU Dritte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80b60035-4845-4c4b-b17a-5af65f27f9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 503 seconds.\n",
      "Rate limit exceeded. Sleeping for 502 seconds.\n",
      "Rate limit exceeded. Sleeping for 506 seconds.\n",
      "Rate limit exceeded. Sleeping for 504 seconds.\n",
      "Rate limit exceeded. Sleeping for 508 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Scrapen von hat 75.6728449344635 Minuten gebraucht.\n",
      "1526\n",
      "15251\n"
     ]
    }
   ],
   "source": [
    "# time counter\n",
    "start = time.time()\n",
    "\n",
    "spd_tweets = []\n",
    "\n",
    "# exclude retweets, just retrieve German Tweets\n",
    "# Zeit ist YYMMDD\n",
    "# Soll ich Zeitraum seit Corona nehmen? also ab 01.01.2020? \n",
    "for response in tweepy.Paginator(client.search_all_tweets, \n",
    "                                query = f'({csu_polis_3}) -is:retweet lang:de',\n",
    "                                user_fields = ['username', 'public_metrics', 'description', 'location'],\n",
    "                                tweet_fields = ['created_at', 'geo', 'public_metrics', 'text'],\n",
    "                                expansions = 'author_id',\n",
    "                                start_time = '2020-01-01T00:00:00Z',\n",
    "                                end_time = '2022-12-05T00:00:00Z'):\n",
    "    time.sleep(1)\n",
    "    spd_tweets.append(response)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Das Scrapen von hat {(end - start)/60} Minuten gebraucht.\")\n",
    "print(len(spd_tweets))\n",
    "\n",
    "result = []\n",
    "user_dict = {}\n",
    "# Loop through each response object\n",
    "for response in spd_tweets:\n",
    "    # Take all of the users, and put them into a dictionary of dictionaries with the info we want to keep\n",
    "    for user in response.includes['users']:\n",
    "        user_dict[user.id] = {'username': user.username, \n",
    "                              'followers': user.public_metrics['followers_count'],\n",
    "                              'tweets': user.public_metrics['tweet_count'],\n",
    "                              'description': user.description,\n",
    "                              'location': user.location\n",
    "                             }\n",
    "    for tweet in response.data:\n",
    "        # For each tweet, find the author's information\n",
    "        author_info = user_dict[tweet.author_id]\n",
    "        # Put all of the information we want to keep in a single dictionary for each tweet\n",
    "        result.append({'author_id': tweet.author_id, \n",
    "                       'username': author_info['username'],\n",
    "                       'author_followers': author_info['followers'],\n",
    "                       'author_tweets': author_info['tweets'],\n",
    "                       'author_description': author_info['description'],\n",
    "                       'author_location': author_info['location'],\n",
    "                       'text': tweet.text,\n",
    "                       'created_at': tweet.created_at,\n",
    "                       'quote_count': tweet.public_metrics['quote_count'],\n",
    "                       'retweets': tweet.public_metrics['retweet_count'],\n",
    "                       'replies': tweet.public_metrics['reply_count'],\n",
    "                       'likes': tweet.public_metrics['like_count'],\n",
    "                      })\n",
    "\n",
    "# Change this list of dictionaries into a dataframe\n",
    "df = pd.DataFrame(result)\n",
    "\n",
    "df[\"Partei\"] = \"CSU/CDU\"\n",
    "\n",
    "df.to_csv(\"csucdu_dritte_40.csv\")\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4318e3e-ce27-4099-9954-94402eea192e",
   "metadata": {},
   "source": [
    "# Fraktionslos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "356767fc-7163-4605-834e-acfddc67c0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "frakt = politicians.loc[politicians.party == \" fraktionslos\"]\n",
    "len(frakt)\n",
    "\n",
    "frakt_polis = []\n",
    "\n",
    "for user in frakt.username:\n",
    "    user = user.replace(\"@\",\"\")\n",
    "    frakt_polis.append(user)\n",
    "    \n",
    "frakt_polis = \" OR \".join([f\"from:{username}\" for username in frakt_polis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4e5d58d-c2f6-43f9-9512-3cb8617ba1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 511 seconds.\n",
      "Rate limit exceeded. Sleeping for 513 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Scrapen von hat 31.800636529922485 Minuten gebraucht.\n",
      "684\n",
      "6838\n"
     ]
    }
   ],
   "source": [
    "# time counter\n",
    "start = time.time()\n",
    "\n",
    "spd_tweets = []\n",
    "\n",
    "# exclude retweets, just retrieve German Tweets\n",
    "# Zeit ist YYMMDD\n",
    "# Soll ich Zeitraum seit Corona nehmen? also ab 01.01.2020? \n",
    "for response in tweepy.Paginator(client.search_all_tweets, \n",
    "                                query = f'({frakt_polis}) -is:retweet lang:de',\n",
    "                                user_fields = ['username', 'public_metrics', 'description', 'location'],\n",
    "                                tweet_fields = ['created_at', 'geo', 'public_metrics', 'text'],\n",
    "                                expansions = 'author_id',\n",
    "                                start_time = '2020-01-01T00:00:00Z',\n",
    "                                end_time = '2022-12-05T00:00:00Z'):\n",
    "    time.sleep(1)\n",
    "    spd_tweets.append(response)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Das Scrapen von hat {(end - start)/60} Minuten gebraucht.\")\n",
    "print(len(spd_tweets))\n",
    "\n",
    "result = []\n",
    "user_dict = {}\n",
    "# Loop through each response object\n",
    "for response in spd_tweets:\n",
    "    # Take all of the users, and put them into a dictionary of dictionaries with the info we want to keep\n",
    "    for user in response.includes['users']:\n",
    "        user_dict[user.id] = {'username': user.username, \n",
    "                              'followers': user.public_metrics['followers_count'],\n",
    "                              'tweets': user.public_metrics['tweet_count'],\n",
    "                              'description': user.description,\n",
    "                              'location': user.location\n",
    "                             }\n",
    "    for tweet in response.data:\n",
    "        # For each tweet, find the author's information\n",
    "        author_info = user_dict[tweet.author_id]\n",
    "        # Put all of the information we want to keep in a single dictionary for each tweet\n",
    "        result.append({'author_id': tweet.author_id, \n",
    "                       'username': author_info['username'],\n",
    "                       'author_followers': author_info['followers'],\n",
    "                       'author_tweets': author_info['tweets'],\n",
    "                       'author_description': author_info['description'],\n",
    "                       'author_location': author_info['location'],\n",
    "                       'text': tweet.text,\n",
    "                       'created_at': tweet.created_at,\n",
    "                       'quote_count': tweet.public_metrics['quote_count'],\n",
    "                       'retweets': tweet.public_metrics['retweet_count'],\n",
    "                       'replies': tweet.public_metrics['reply_count'],\n",
    "                       'likes': tweet.public_metrics['like_count'],\n",
    "                      })\n",
    "\n",
    "# Change this list of dictionaries into a dataframe\n",
    "df = pd.DataFrame(result)\n",
    "\n",
    "df[\"Partei\"] = \"fraktionslos\"\n",
    "\n",
    "df.to_csv(\"fraktionslos.csv\")\n",
    "print(len(df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
